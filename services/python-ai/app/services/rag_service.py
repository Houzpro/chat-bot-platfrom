"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π RAG —Å–µ—Ä–≤–∏—Å –¥–ª—è –ª—é–±—ã—Ö —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
–ë–µ–∑ —Ö–∞—Ä–¥–∫–æ–¥–∞, –ø–æ–ª–∞–≥–∞–µ—Ç—Å—è –Ω–∞ –º–æ—â–Ω—ã–µ embedding –∏ reranking –º–æ–¥–µ–ª–∏
"""
import threading
import re
from typing import List, Dict, Any, Optional

from sentence_transformers import SentenceTransformer, CrossEncoder
import nltk

from app.config.settings import settings


# –°–∫–∞—á–∞—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è NLTK
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    try:
        nltk.download('punkt', quiet=True)
    except Exception:
        pass


def clean_pdf_artifacts(text: str) -> str:
    """
    –£–¥–∞–ª–∏—Ç—å –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –∏ –º—É—Å–æ—Ä –∏–∑ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω–æ–≥–æ PDF
    """
    text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F]', '', text)
    text = re.sub(r'[^\w\s\-\.,!?:;()&\'"\n]', '', text, flags=re.UNICODE)
    text = re.sub(r' +', ' ', text)
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text.strip()



class RAGService:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π RAG —Å–µ—Ä–≤–∏—Å:
    - –†–∞–±–æ—Ç–∞–µ—Ç —Å –ª—é–±—ã–º–∏ —Ç–∏–ø–∞–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    - –ë–µ–∑ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–≥–æ —Ö–∞—Ä–¥–∫–æ–¥–∞
    - –ü–æ–ª–∞–≥–∞–µ—Ç—Å—è –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ embedding –∏ reranking –º–æ–¥–µ–ª–µ–π
    """
    
    def __init__(self):
        self._embedding_model = None
        self._reranker_model = None
        self._lock = threading.Lock()
    
    def load_embedding_model(self) -> SentenceTransformer:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è embeddings (singleton)"""
        with self._lock:
            if self._embedding_model is None:
                if not settings.embedding_model_name:
                    raise ValueError("EMBEDDING_MODEL_NAME is not configured")
                if not settings.embedding_cache_folder:
                    raise ValueError("EMBEDDING_CACHE_FOLDER is not configured")
                
                self._embedding_model = SentenceTransformer(
                    settings.embedding_model_name,
                    cache_folder=settings.embedding_cache_folder
                )
                print(f"‚úÖ Embedding model loaded: {settings.embedding_model_name}")
            return self._embedding_model
    
    def load_reranker_model(self) -> Optional[CrossEncoder]:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å Cross-Encoder reranker (singleton)"""
        with self._lock:
            if self._reranker_model is None and settings.use_reranker:
                try:
                    model_name = settings.reranker_model_name or "cross-encoder/ms-marco-MiniLM-L-6-v2"
                    self._reranker_model = CrossEncoder(model_name, max_length=512)
                    print(f"‚úÖ Reranker loaded: {model_name}")
                except Exception as e:
                    print(f"‚ö†Ô∏è Failed to load reranker: {e}")
                    return None
            return self._reranker_model
    
    def create_embeddings(self, texts: List[str], is_query: bool = False) -> List[List[float]]:
        """
        –°–æ–∑–¥–∞—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            is_query: True –µ—Å–ª–∏ —ç—Ç–æ –∑–∞–ø—Ä–æ—Å (–¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –ø—Ä–µ—Ñ–∏–∫—Å)
        """
        embedding_model = self.load_embedding_model()
        
        # –î–ª—è multilingual-e5 –º–æ–¥–µ–ª–µ–π –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å
        model_name = settings.embedding_model_name or ""
        if "e5" in model_name.lower():
            if is_query:
                texts = [f"query: {text}" for text in texts]
            else:
                texts = [f"passage: {text}" for text in texts]
        
        embeddings = embedding_model.encode(
            texts,
            convert_to_numpy=True,
            normalize_embeddings=True,
            show_progress_bar=False
        )
        
        return embeddings.tolist()
    

    def split_text_semantic(self, text: str, chunk_size: int = 2500, overlap: int = 500) -> List[str]:
        """
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ:
        - –†–∞–±–æ—Ç–∞–µ—Ç —Å –ª—é–±—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ (–Ω–µ –ø—Ä–∏–≤—è–∑–∞–Ω–æ –∫ –≥–µ—Ä–æ—è–º/—Å–ø–µ—Ü–∏—Ñ–∏–∫–µ)
        - –î–µ—Ä–∂–∏—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –ª–æ–∫–∞–ª—å–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤/—Å–µ–∫—Ü–∏–π, –Ω–æ –Ω–µ —Å–º–µ—à–∏–≤–∞–µ—Ç —Å–µ–∫—Ü–∏–∏
        - –î–µ–ª–∞–µ—Ç overlap —Ç–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–π —Å–µ–∫—Ü–∏–∏, —á—Ç–æ–±—ã –Ω–µ —Ç—è–Ω—É—Ç—å —á—É–∂–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        - –ß–∏—Å—Ç–∏—Ç PDF –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã
        """
        text = clean_pdf_artifacts(text)
        if not text or len(text) <= chunk_size:
            return [text] if text else []

        lines = text.split('\n')
        chunks: List[str] = []
        current_chunk: List[str] = []
        current_chunk_len = 0
        last_header = ""

        def is_heading(line: str) -> bool:
            if len(line) > 180:
                return False
            if line.startswith('#'):
                return True
            # ALL CAPS short lines
            if line.isupper() and len(line.split()) <= 8:
                return True
            # Numbered / bullet headings
            if re.match(r"^(\d+\.|[ivxlcdm]+\.|[‚Ä¢\-*])\s+", line.strip(), flags=re.IGNORECASE):
                return len(line) <= 160
            # Title case short phrase
            if line and line[0].isupper() and len(line.split()) <= 6:
                return True
            return False

        for raw_line in lines:
            line = raw_line.strip()
            if not line:
                continue

            heading = is_heading(line)

            # –ï—Å–ª–∏ –Ω–æ–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —É–∂–µ –µ—Å—Ç—å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π —á–∞–Ω–∫ ‚Äî –∑–∞–∫—Ä—ã–≤–∞–µ–º —á–∞–Ω–∫
            if heading and current_chunk:
                if last_header and last_header not in ' '.join(current_chunk):
                    chunk_text = f"{last_header}\n{' '.join(current_chunk)}"
                else:
                    chunk_text = ' '.join(current_chunk)
                chunks.append(chunk_text)
                # –ù–µ –ø–µ—Ä–µ–Ω–æ—Å–∏–º overlap –º–µ–∂–¥—É —Å–µ–∫—Ü–∏—è–º–∏
                current_chunk = []
                current_chunk_len = 0

            if heading:
                last_header = line
                sentences = [line]
            else:
                try:
                    sentences = nltk.sent_tokenize(line)
                except Exception:
                    sentences = [line]

            for sent in sentences:
                sent = sent.strip()
                if not sent or len(sent) < 3:
                    continue

                sent_with_context = sent
                if not heading and last_header:
                    sent_with_context = f"{last_header}: {sent}"

                new_len = current_chunk_len + len(sent_with_context) + 1

                if new_len > chunk_size and current_chunk:
                    if last_header and last_header not in ' '.join(current_chunk):
                        chunk_text = f"{last_header}\n{' '.join(current_chunk)}"
                    else:
                        chunk_text = ' '.join(current_chunk)

                    chunks.append(chunk_text)

                    overlap_buf: List[str] = []
                    # Overlap —Ç–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–∏ —Å–µ–∫—Ü–∏–∏ (–Ω–µ –∫–æ–ø–∏–º –ø—Ä–∏ –∑–∞–≥–æ–ª–æ–≤–∫–µ)
                    if overlap > 0 and not heading:
                        tail = chunk_text[-overlap:]
                        overlap_buf.append(tail)

                    current_chunk = overlap_buf
                    current_chunk_len = sum(len(x) for x in overlap_buf)

                current_chunk.append(sent_with_context)
                current_chunk_len = new_len

        if current_chunk:
            if last_header and last_header not in ' '.join(current_chunk):
                chunk_text = f"{last_header}\n{' '.join(current_chunk)}"
            else:
                chunk_text = ' '.join(current_chunk)
            chunks.append(chunk_text)

        return chunks if chunks else [text]
    
    def rerank_documents(
        self,
        query: str,
        documents: List[Dict[str, Any]],
        top_k: int = 20
    ) -> List[Dict[str, Any]]:
        """
        Reranking —Å –ø–æ–º–æ—â—å—é CrossEncoder
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –ª—É—á—à–µ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        
        Args:
            query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            documents: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
            top_k: –°–∫–æ–ª—å–∫–æ –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–µ—Ä–Ω—É—Ç—å
        """
        if not documents:
            return []
        
        reranker = self.load_reranker_model()
        if reranker is None:
            print("‚ö†Ô∏è Reranker not available, returning original order")
            return documents[:top_k]
        
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä—ã (query, document)
            pairs = []
            for doc in documents:
                doc_text = doc.get('text', '')[:2000]  # –ü–µ—Ä–≤—ã–µ 2000 —Å–∏–º–≤–æ–ª–æ–≤
                if doc_text.strip():
                    pairs.append((query, doc_text))
                else:
                    # –ï—Å–ª–∏ –Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    pairs.append((query, str(doc)))
            
            if not pairs:
                return documents[:top_k]
            
            # –í—ã—á–∏—Å–ª—è–µ–º scores —á–µ—Ä–µ–∑ CrossEncoder
            scores = reranker.predict(pairs, show_progress_bar=False)
            
            # –î–æ–±–∞–≤–ª—è–µ–º scores –≤ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            for i, doc in enumerate(documents):
                if i < len(scores):
                    doc['rerank_score'] = float(scores[i])
                else:
                    doc['rerank_score'] = 0.0
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ rerank_score
            reranked = sorted(documents, key=lambda d: d.get('rerank_score', 0), reverse=True)
            
            # –í—ã–≤–æ–¥–∏–º –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç reranking'–∞
            print(f"\nüìä RERANKING RESULTS for query: '{query}'")
            print(f"   Total candidates: {len(documents)}")
            print(f"   Returning: {min(top_k, len(reranked))}")
            print(f"\n   Top results:")
            for i, doc in enumerate(reranked[:top_k]):
                score = doc.get('rerank_score', 0)
                text_preview = doc.get('text', '')[:100].replace('\n', ' ')
                file_name = doc.get('file_name', 'unknown')
                chunk_idx = doc.get('chunk_index', '?')
                print(f"   #{i+1}: score={score:7.4f} | {file_name}[{chunk_idx}] | \"{text_preview}...\"")
            
            return reranked[:top_k]
        
        except Exception as e:
            print(f"‚ö†Ô∏è Reranking failed: {e}")
            import traceback
            traceback.print_exc()
            # Fallback: –≤–µ—Ä–Ω—ë–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
            return documents[:top_k]
    
    def advanced_search(
        self,
        bot_id: str,
        query: str,
        vector_results: List[Dict[str, Any]],
        top_k: int = 30
    ) -> List[Dict[str, Any]]:
        """
        –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–∏—Å–∫: dense retrieval + optional reranking
        
        –°—Ç—Ä–∞—Ç–µ–≥–∏—è:
        1. –ë–µ—Ä—ë–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        2. –ï—Å–ª–∏ reranker –≤–∫–ª—é—á–µ–Ω - –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä
        3. –í–æ–∑–≤—Ä–∞—â–∞–µ–º top-k –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        Args:
            bot_id: ID –±–æ—Ç–∞ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            vector_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏–∑ Qdrant
            top_k: –°–∫–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–µ—Ä–Ω—É—Ç—å
        """
        print(f"\nüîç ADVANCED SEARCH")
        print(f"   Query: '{query}'")
        print(f"   Bot: {bot_id}")
        print(f"   Vector results received: {len(vector_results)}")
        
        # –î–µ–¥—É–±–ª–∏–∫–∞—Ü–∏—è –ø–æ ID
        all_candidates = {}
        for doc in vector_results:
            doc_id = str(doc.get('id', ''))
            if doc_id and doc_id not in all_candidates:
                all_candidates[doc_id] = doc
        
        candidates_list = list(all_candidates.values())
        print(f"   After dedup: {len(candidates_list)} unique candidates")
        
        if not candidates_list:
            print("‚ö†Ô∏è No candidates found after dedup")
            return []
        
        # –ï—Å–ª–∏ reranker –≤–∫–ª—é—á–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        if settings.use_reranker:
            print(f"   Using reranker: {settings.reranker_model_name}")
            # –ë–µ—Ä—ë–º –±–æ–ª—å—à–µ –¥–ª—è —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è —á—Ç–æ–±—ã –Ω–µ –ø–æ—Ç–µ—Ä—è—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ
            rerank_k = min(top_k * 3, len(candidates_list))
            print(f"   Reranking {rerank_k} candidates...")
            reranked = self.rerank_documents(query, candidates_list, top_k=rerank_k)
            print(f"   ‚úÖ Returned {len(reranked[:top_k])} results after reranking")
            return reranked[:top_k]
        else:
            # –ë–µ–∑ reranker –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–æ—Å—Ç–æ top-k –ø–æ –∏—Å—Ö–æ–¥–Ω–æ–º—É –ø–æ—Ä—è–¥–∫—É (distance)
            print(f"   ‚ö†Ô∏è Reranker disabled, returning {min(top_k, len(candidates_list))} by distance")
            return candidates_list[:top_k]
    
    def build_context(
        self,
        query: str,
        documents: List[Dict[str, Any]],
        max_chars: int = 120000,
        max_docs: int = 30,
        min_docs: int = 8
    ) -> str:
        """
        –°–æ–±—Ä–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ LLM
        
        –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –æ—Ç–¥–∞—ë–º –í–°–Å —á—Ç–æ –Ω–∞—à–ª–∏, LLM —Å–∞–º–∞ –≤—ã–±–µ—Ä–µ—Ç –Ω—É–∂–Ω–æ–µ
        
        Args:
            query: –ó–∞–ø—Ä–æ—Å (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –æ—Å—Ç–∞–≤–ª–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
            documents: –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            max_chars: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        """
        if not documents:
            return ""
        def extract_keywords(text: str) -> List[str]:
            # –ë–µ—Ä—ë–º –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª–∏–Ω–æ–π 4+ —Å–∏–º–≤–æ–ª–æ–≤ (—Ä—É—Å/–ª–∞—Ç), —á—Ç–æ–±—ã –æ—Ç—Å–µ—á—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            return [w.lower() for w in re.findall(r"[A-Za-z–ê-–Ø–∞-—è–Å—ë']+", text) if len(w) >= 4]

        query_keywords = extract_keywords(query)

        main_header: Optional[str] = None
        first_text = documents[0].get('text', '')
        first_line = first_text.split('\n', 1)[0].strip()
        if first_line and len(first_line) <= 60:
            main_header = first_line

        filtered_docs: List[Dict[str, Any]] = []
        max_rerank = 0.0
        for doc in documents:
            score = float(doc.get('rerank_score', 0) or 0)
            if score > max_rerank:
                max_rerank = score

        for doc in documents:
            text = doc.get('text', '')
            lower_text = text.lower()
            score = float(doc.get('rerank_score', 0) or 0)

            keep = False
            if query_keywords and any(k in lower_text for k in query_keywords):
                keep = True
            if not keep and main_header and main_header.lower() in lower_text:
                keep = True
            if not filtered_docs:
                keep = True

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–æ rerank_score, –Ω–æ –Ω–µ —Ä–µ–∂–µ–º –ø–µ—Ä–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
            if keep and max_rerank > 0 and filtered_docs:
                # –û—Å—Ç–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –ø—Ä–∏–µ–º–ª–µ–º—ã–º —Å–∫–æ—Ä–æ–º, —á—Ç–æ–±—ã —É–±—Ä–∞—Ç—å —à—É–º
                if score < max_rerank * 0.55:
                    keep = False

            if keep:
                filtered_docs.append(doc)

        # –ï—Å–ª–∏ —Ñ–∏–ª—å—Ç—Ä –¥–∞–ª –º–∞–ª–æ ‚Äî –¥–æ–±–∞–≤–∏–º —Å–≤–µ—Ä—Ö—É –æ—Å—Ç–∞–≤—à–∏–µ—Å—è (—Ä–∞—Å—à–∏—Ä—è–µ–º recall), —Å–æ—Ö—Ä–∞–Ω—è—è –∏—Å—Ö–æ–¥–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫
        if len(filtered_docs) < min_docs:
            for doc in documents:
                if doc in filtered_docs:
                    continue
                filtered_docs.append(doc)
                if len(filtered_docs) >= min_docs:
                    break

        # –ï—Å–ª–∏ —Å–æ–≤—Å–µ–º –Ω–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –∏ –∑–∞–≥–æ–ª–æ–≤–∫—É ‚Äîfallback: –±–µ—Ä—ë–º —Ç–æ–ø min_docs –∏–∑ rerank
        if not filtered_docs:
            filtered_docs = documents[:min(min_docs, len(documents))]

        # –£–¥–∞–ª—è–µ–º —Ç–æ—á–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã —Ç–µ–∫—Å—Ç–∞, —á—Ç–æ–±—ã –Ω–µ —Ä–∞–∑–¥—É–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
        deduped: List[Dict[str, Any]] = []
        seen_texts = set()
        for doc in filtered_docs:
            text = doc.get('text', '')
            key = text.strip()
            if key in seen_texts:
                continue
            seen_texts.add(key)
            deduped.append(doc)
        filtered_docs = deduped

        print(f"üßπ Context filter: {len(filtered_docs)} kept of {len(documents)} (min_docs={min_docs})")

        context_parts = []
        total_chars = 0

        for doc in filtered_docs:
            text = doc.get('text', '').strip()
            if not text:
                continue

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≥–æ–ª—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –±–µ–∑ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è (–µ—Å–ª–∏ —ç—Ç–æ –Ω–µ —Å–∞–º—ã–π –ø–µ—Ä–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç)
            if len(text) < 120 and '.' not in text and '\n' not in text and len(context_parts) > 0:
                continue

            if len(context_parts) >= max_docs:
                break

            if total_chars + len(text) > max_chars:
                remaining = max_chars - total_chars
                if remaining > 500:
                    text = text[:remaining] + "..."
                    context_parts.append(text)
                break

            context_parts.append(text)
            total_chars += len(text)
        
        context = '\n\n'.join(context_parts)
        print(f"üìÑ Context built: {len(context_parts)} documents, {total_chars} chars")
        
        return context
    
    def build_bm25_index(self, bot_id: str, documents: List[Dict[str, Any]]) -> None:
        """
        Placeholder –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ (BM25 –±–æ–ª—å—à–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –Ω–æ–≤–æ–º pipeline)
        –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ dense retrieval + reranking
        
        Args:
            bot_id: ID –±–æ—Ç–∞
            documents: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è)
        """
        print(f"‚ö†Ô∏è build_bm25_index called for bot {bot_id}, but BM25 is deprecated")
        print(f"   Using modern dense retrieval + reranking instead")
        pass  # No-op: —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π pipeline –Ω–µ –Ω—É–∂–¥–∞–µ—Ç—Å—è –≤ BM25


# Singleton instance
rag_service = RAGService()
