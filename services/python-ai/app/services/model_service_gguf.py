"""
–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å GGUF –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ llama.cpp
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è CPU - –≤ 10-20 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ PyTorch
"""
import threading
from pathlib import Path
from typing import Iterator, Optional, Any, List, Dict

try:
    from llama_cpp import Llama
    LLAMA_CPP_AVAILABLE = True
except ImportError:
    LLAMA_CPP_AVAILABLE = False
    Llama = None  # type: ignore

from app.config.settings import settings


class ModelServiceGGUF:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å GGUF –º–æ–¥–µ–ª—è–º–∏ (CPU –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)"""
    
    def __init__(self):
        self._model: Optional[Any] = None
        self._lock = threading.Lock()
        self._stop_sequences = settings.generation_stop_sequences
        
        if not LLAMA_CPP_AVAILABLE:
            print("‚ö†Ô∏è  llama-cpp-python –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install llama-cpp-python")
    
    def load_model(self) -> Any:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ GGUF –º–æ–¥–µ–ª–∏ (singleton)
        
        Returns:
            Llama model instance –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω llama-cpp-python
        """
        if not LLAMA_CPP_AVAILABLE:
            raise ImportError("llama-cpp-python –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        
        with self._lock:
            if self._model is not None:
                return self._model
            
            model_path = settings.gguf_model_path
            if model_path is None:
                raise ValueError("GGUF_MODEL_PATH is not configured")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
            if not Path(model_path).exists():
                raise FileNotFoundError(
                    f"GGUF –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}\n"
                    f"–°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å:\n"
                    f"cd models && curl -L -o qwen2.5-3b-instruct-q4_k_m.gguf https://huggingface.co/Qwen/Qwen2.5-3B-Instruct-GGUF/resolve/main/qwen2.5-3b-instruct-q4_k_m.gguf"
                )
            
            print(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ GGUF –º–æ–¥–µ–ª–∏ –∏–∑ {model_path}...")
            print(f"‚ö° CPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: {settings.n_threads} –ø–æ—Ç–æ–∫–æ–≤")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
            from llama_cpp import Llama as LlamaCpp
            self._model = LlamaCpp(
                model_path=model_path,
                n_ctx=settings.n_ctx,
                n_threads=settings.n_threads,
                n_gpu_layers=0,  # –î–ª—è CPU –≤—Å–µ–≥–¥–∞ 0
                verbose=False,
                n_batch=512,
                use_mlock=True  # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç swapping
            )
            
            print(f"‚úÖ GGUF –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (–∫–æ–Ω—Ç–µ–∫—Å—Ç: {settings.n_ctx}, –ø–æ—Ç–æ–∫–∏: {settings.n_threads})")
            return self._model
    
    def generate_response(
        self,
        messages: List[Dict[str, str]],
        max_new_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        top_k: Optional[int] = None,
        do_sample: Optional[bool] = None,
        behavior_instruction: Optional[str] = None,
        system_prompt: Optional[str] = None
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è)
        
        Args:
            messages: –ò—Å—Ç–æ—Ä–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π
            max_new_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            top_p: Nucleus sampling
            top_k: Top-k sampling
            do_sample: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å sampling
            behavior_instruction: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        """
        model = self.load_model()
        
        prompt = self._format_messages(messages, behavior_instruction, system_prompt)
        gen_kwargs = self._prepare_generation_kwargs(
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            do_sample=do_sample
        )

        output = model(prompt, stream=False, stop=self._stop_sequences, **gen_kwargs)
        
        return output['choices'][0]['text'].strip()
    
    def generate_response_stream(
        self,
        messages: List[Dict[str, str]],
        max_new_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        top_k: Optional[int] = None,
        do_sample: Optional[bool] = None,
        behavior_instruction: Optional[str] = None,
        system_prompt: Optional[str] = None
    ) -> Iterator[str]:
        """
        –ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        
        Args:
            messages: –ò—Å—Ç–æ—Ä–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π
            max_new_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            top_p: Nucleus sampling
            top_k: Top-k sampling
            do_sample: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å sampling
            behavior_instruction: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        
        Yields:
            –¢–æ–∫–µ–Ω—ã –æ—Ç–≤–µ—Ç–∞
        """
        model = self.load_model()
        
        # Debug logging
        if system_prompt:
            print(f"[AI Service] System prompt length: {len(system_prompt)} chars")
            print(f"[AI Service] System prompt preview: {system_prompt[:500]}...")
        print(f"[AI Service] User query: {messages[0].get('content', '') if messages else 'N/A'}")
        
        prompt = self._format_messages(messages, behavior_instruction, system_prompt)
        gen_kwargs = self._prepare_generation_kwargs(
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            do_sample=do_sample
        )

        started = False  # –§–ª–∞–≥ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –Ω–∞—á–∞–ª–∞ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        for output in model(prompt, stream=True, stop=self._stop_sequences, **gen_kwargs):
            token = output['choices'][0]['text']
            # –£–¥–∞–ª—è–µ–º –æ—Ç–∫—Ä—ã–≤–∞—é—â–∏–µ/–∑–∞–∫—Ä—ã–≤–∞—é—â–∏–µ —Ç–µ–≥–∏, –∫–æ–Ω—Ç–µ–Ω—Ç –æ—Å—Ç–∞–≤–ª—è–µ–º
            token = token.replace('<think>', '').replace('</think>', '')
            token = token.replace('<thought>', '').replace('</thought>', '')
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Ç–æ–∫–µ–Ω—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –≤ –Ω–∞—á–∞–ª–µ –æ—Ç–≤–µ—Ç–∞
            if not started:
                if not token or token.isspace():
                    continue
                started = True
            
            # –û—Ç–¥–∞–µ–º –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –ø–æ—Å–ª–µ –Ω–∞—á–∞–ª–∞ (–≤–∞–∂–Ω–æ –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)
            yield token
    
    def _format_messages(
        self,
        messages: List[Dict[str, str]],
        behavior_instruction: Optional[str] = None,
        system_prompt: Optional[str] = None
    ) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ Qwen chat —Ñ–æ—Ä–º–∞—Ç
        
        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
            behavior_instruction: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        
        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π prompt
        """
        # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        if system_prompt:
            # User provided custom prompt - combine with base
            system_message = f"{system_prompt}\n\n{settings.generation_system_base_prompt}"
        else:
            # Use default prompts
            system_message = f"{settings.generation_user_prompt}\n\n{settings.generation_system_base_prompt}"
            if behavior_instruction:
                system_message = f"{behavior_instruction}\n{system_message}"
        
        # Qwen —Ñ–æ—Ä–º–∞—Ç: <|im_start|>role\ncontent<|im_end|>
        prompt = f"<|im_start|>system\n{system_message}<|im_end|>\n"
        
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            prompt += f"<|im_start|>{role}\n{content}<|im_end|>\n"
        
        prompt += "<|im_start|>assistant\n"
        return prompt

    def _prepare_generation_kwargs(
        self,
        max_new_tokens: Optional[int],
        temperature: Optional[float],
        top_p: Optional[float],
        top_k: Optional[int],
        do_sample: Optional[bool]
    ) -> Dict[str, Any]:
        """–ü—Ä–∏–≤–æ–¥–∏—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫ –∑–Ω–∞—á–µ–Ω–∏—è–º –∏–∑ settings –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –≤—Ö–æ–¥–Ω—ã—Ö."""
        max_tokens = settings.generation_max_new_tokens if max_new_tokens is None else max_new_tokens
        sample_flag = settings.generation_do_sample if do_sample is None else do_sample
        temperature_val = settings.generation_temperature if temperature is None else temperature
        top_p_val = settings.generation_top_p if top_p is None else top_p
        top_k_val = settings.generation_top_k if top_k is None else top_k

        return {
            "max_tokens": max_tokens,
            "temperature": 0.0 if not sample_flag else temperature_val,
            "top_p": 1.0 if not sample_flag else top_p_val,
            "top_k": -1 if not sample_flag else top_k_val
        }


# Singleton instance
model_service_gguf = ModelServiceGGUF()
